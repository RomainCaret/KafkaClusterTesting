{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Writing our Producer\n",
        "First of all, we are creating a yml file. This file will be a description of our azure ml component. It explains how this component works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1672676428554
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "component_dir = \"./components\"\n",
        "os.makedirs(component_dir, exist_ok=True)\n",
        "\n",
        "src_dir = \"./components/src_producer\"\n",
        "os.makedirs(src_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/producer.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile $component_dir/producer.yml\n",
        "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
        "name: producer\n",
        "display_name: producer\n",
        "is_deterministic: false\n",
        "version: 0.71\n",
        "type: command\n",
        "inputs:\n",
        "  num_producer:\n",
        "    type: string\n",
        "    description: \"numero du producer\"\n",
        "    default: \"Producer\"\n",
        "  num_machine:\n",
        "    type: string\n",
        "    description: \"numero de la machine\"\n",
        "    default: \"Machine\"\n",
        "  brokerAddress:\n",
        "    type: string\n",
        "    description: \"Broker address\"\n",
        "  machineKafka:\n",
        "    type: string\n",
        "    description: \"Machine Kafka\"\n",
        "    default: \"m1\"\n",
        "code: ./src_producer\n",
        "environment: azureml:kafka-custom-env@latest\n",
        "command: >-\n",
        "  python producer.py\n",
        "  --num_producer ${{inputs.num_producer}}\n",
        "  --num_machine ${{inputs.num_machine}}\n",
        "  --brokerAddress ${{inputs.brokerAddress}}\n",
        "  --machineKafka ${{inputs.machineKafka}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./components/src_producer/producer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {src_dir}/producer.py\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "import mlflow\n",
        "import datetime, warnings, scipy\n",
        "from pathlib import Path\n",
        "import psutil\n",
        "import platform\n",
        "from datetime import datetime\n",
        "import cpuinfo\n",
        "import socket\n",
        "import uuid\n",
        "import re\n",
        "from kafka import KafkaProducer\n",
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import threading\n",
        "\n",
        "# Create random data of characters\n",
        "amount_of_ko = 10\n",
        "load_message = ''.join(random.choice('0123456789ABCDEF') for i in range(amount_of_ko*1024))\n",
        "\n",
        "# Create the data of the producer proc\n",
        "def create_producer_proc_data():\n",
        "    proc = {}\n",
        "    uname = platform.uname()\n",
        "    proc['system'] = uname.system\n",
        "    proc['processor'] = uname.processor\n",
        "    proc['cpu_brand'] = cpuinfo.get_cpu_info()['brand_raw']\n",
        "    proc['cpu_hz'] = cpuinfo.get_cpu_info()['hz_actual_friendly']\n",
        "    proc['cpu_cores'] = psutil.cpu_count(logical=False)\n",
        "    proc['cpu_cores_total'] = psutil.cpu_count(logical=True)\n",
        "    svmem = psutil.virtual_memory()\n",
        "    proc['ram_total'] = svmem.total\n",
        "    return proc\n",
        "\n",
        "# Create the data to send\n",
        "def create_data_json(proc_data,machineKafka, flow, batch_size):\n",
        "    data = {}\n",
        "    # put 5 ko of data\n",
        "    data['data'] = load_message\n",
        "    data['proc_prod'] = proc_data\n",
        "    data['machine_kafka'] = machineKafka\n",
        "    data['batch_size'] = batch_size\n",
        "    data['timestamp'] = time.time()\n",
        "    return data\n",
        "\n",
        "def print_debug(message, brokerAddress):\n",
        "    producer = KafkaProducer(\n",
        "\t\t\tbootstrap_servers=brokerAddress,\n",
        "\t\t\tapi_version=(0, 10, 1),\n",
        "\t\t\tacks=1,)\n",
        "    data = {\"message\": message}\n",
        "    data_encode = json.dumps(data).encode('utf-8')\n",
        "    producer.send(\"debug\", data_encode)\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "# Send data\n",
        "def send_data(brokerAddress, num_messages, topic, proc_data, machineKafka, batch_size):\n",
        "    \n",
        "    \n",
        "    producer = KafkaProducer(\n",
        "            bootstrap_servers=brokerAddress,\n",
        "            api_version=(0, 10, 1),\n",
        "            acks=1,)\n",
        "    data = create_data_json(proc_data, machineKafka, num_messages, batch_size)\n",
        "    data_encode = json.dumps(data).encode('utf-8')\n",
        "    \n",
        "    for _ in range(num_messages):\n",
        "        producer.send(topic, data_encode)\n",
        "\n",
        "    \n",
        "\n",
        "    # Flush and close the producer to ensure all messages are sent\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "# Send data secured\n",
        "def send_data_secured(brokerAddress, num_messages, topic):\n",
        "\n",
        "    producer = KafkaProducer(\n",
        "            bootstrap_servers=brokerAddress,\n",
        "            api_version=(0, 10, 1),\n",
        "            acks=1,)\n",
        "    data = {}\n",
        "    data_encode = json.dumps(data).encode('utf-8')\n",
        "\n",
        "    for _ in range(num_messages):\n",
        "        producer.send(topic, data_encode)\n",
        "    \n",
        "    print_debug(\"end topic: \" + topic, brokerAddress)\n",
        "    # Flush and close the producer to ensure all messages are sent\n",
        "    producer.flush()\n",
        "    producer.close()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--num_producer\", type=str, help=\"numero du producer\")\n",
        "    parser.add_argument(\"--num_machine\", type=str, help=\"numero de la machine\")\n",
        "    parser.add_argument(\"--brokerAddress\", type=str, help=\"Broker address\")\n",
        "    parser.add_argument(\"--machineKafka\", type=str, help=\"Machine Kafka\")\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "\n",
        "    name = args.num_machine + \"-\" + args.num_producer\n",
        "    print(\"name: \", name)\n",
        "    \n",
        "    # Set the data of the producer\n",
        "    proc_data = create_producer_proc_data()\n",
        "\n",
        "    brokerAddress = args.brokerAddress.split(\" \")\n",
        "    for i in range(len(brokerAddress)):\n",
        "        brokerAddress[i] = brokerAddress[i] + \":9094\"\n",
        "\n",
        "    consumer = KafkaConsumer(\n",
        "            bootstrap_servers=brokerAddress,\n",
        "            api_version=(0, 10),)\n",
        "\n",
        "    consumer.subscribe([\"manager-producer\"])\n",
        "    \n",
        "    # Initialize the variables\n",
        "    stop = False\n",
        "    flow = 0\n",
        "    batch_size = 0\n",
        "    amount_of_time = 0\n",
        "\n",
        "    while True:\n",
        "        for message in consumer:\n",
        "            if json.loads(message.value.decode(\"utf-8\"))[\"stop\"] == True:\n",
        "                stop = True\n",
        "                break\n",
        "            diff = time.time() - json.loads(message.value.decode(\"utf-8\"))[\"timestamp\"]\n",
        "            # If the message is older than 10 minutes, we ignore it\n",
        "            if diff < 600:\n",
        "                print_debug(\"Let s produce: \" + str(json.loads(message.value.decode(\"utf-8\"))), brokerAddress)\n",
        "                flow = json.loads(message.value.decode(\"utf-8\"))[\"flow\"]\n",
        "                batch_size = json.loads(message.value.decode(\"utf-8\"))[\"batch_size\"]\n",
        "                amount_of_time = json.loads(message.value.decode(\"utf-8\"))[\"amount_of_time\"]\n",
        "                break\n",
        "        \n",
        "        # Start the producer with the flow and the batch size given by the manager for the next amount of time\n",
        "        if stop:\n",
        "            break\n",
        "        time_start = time.time()\n",
        "        while time.time() - time_start < amount_of_time:\n",
        "            threading.Thread(target=send_data, args=(brokerAddress, int(flow/2), \"topic-aiops\"+name, proc_data, args.machineKafka, batch_size)).start()\n",
        "            # sleep for 0.5 seconds\n",
        "            time.sleep(0.5)\n",
        "        # Send the end of the flow\n",
        "        time.sleep(1)\n",
        "        send_data_secured(brokerAddress, 1, \"consumer-write\"+name)\n",
        "        time.sleep(10)\n",
        "\n",
        "    send_data_secured(brokerAddress, 1, \"consumer-end\"+name)\n",
        "    time.sleep(20)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "* Go into the right folder\n",
        "```\n",
        "$ cd kafka\n",
        "```\n",
        "* Execute the setenv.sh script that will setup variables environment\n",
        "```\n",
        "$ source setenv.sh\n",
        "```\n",
        "* Go into the components folder \n",
        "```\n",
        "$ cd components\n",
        "```\n",
        "* Login Azure with this command\n",
        "```\n",
        "$ az login --tenant $tenant_id\n",
        "```\n",
        "* After you successfuly login, set the right subscription ID you are currently using in Microsoft Azure Machine learning Studio.\n",
        "```\n",
        "$ az account set --subscription $subscription_id\n",
        "```\n",
        "* Set your workspace and resource group\n",
        "```\n",
        "$ az configure --defaults workspace=$workspace_name group=$resource_group\n",
        "```\n",
        "* You are now perfectly set up and can create a component with this command with the .yml file that we created earlier :\n",
        "```\n",
        "$ az ml component create --file producer.yml\n",
        "```\n",
        "* It should display a JSON with informations of the component you just created"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "azureml_py38",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
