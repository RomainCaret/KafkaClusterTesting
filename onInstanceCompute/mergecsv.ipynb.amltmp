{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Write a csv file with all the scheduled events\n",
        "# use spark to read the csv file and create a dataframe\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "# Get the list of files\n",
        "files = os.listdir('data')\n",
        "\n",
        "# Create a new file\n",
        "for file in files:\n",
        "    if file.startswith(\"data\"):\n",
        "        df = spark.read.csv('data/'+file, header=True)\n",
        "        # change groupID to int\n",
        "        df = df.withColumn(\"groupID\", df[\"groupID\"].cast(\"int\"))\n",
        "        # change average to float\n",
        "        df = df.withColumn(\"average\", df[\"average\"].cast(\"float\"))\n",
        "        # Reduce by groupID and calculate the average and total and keep the other columns\n",
        "        df = df.groupBy('groupID').agg({'system_prod':'first', 'processor_prod':'first', 'cpu_brand_prod':'first', 'cpu_hz_prod':'first', 'cpu_cores_prod':'first', 'cpu_cores_total_prod':'first', 'ram_total_prod':'first', 'pricePerHour_prod':'first', 'system_cons':'first', 'processor_cons':'first', 'cpu_brand_cons':'first', 'cpu_hz_cons':'first', 'cpu_cores_cons':'first', 'cpu_cores_total_cons':'first', 'ram_total_cons':'first', 'pricePerHour_cons':'first', 'average':'avg', 'total':'sum', 'linger_ms':'first', 'batch_size':'first', 'machine_kafka':'first'})\n",
        "        # transform the dataframe to a pandas dataframe\n",
        "        # Remove first/avg/sum and () from the column names\n",
        "        df = df.toPandas()\n",
        "        df.columns = df.columns.str.replace('first', '')\n",
        "        df.columns = df.columns.str.replace('avg', '')\n",
        "        df.columns = df.columns.str.replace('sum', '')\n",
        "        df.columns = df.columns.str.replace('(', '')\n",
        "        df.columns = df.columns.str.replace(')', '')\n",
        "        # order the columns by groupID\n",
        "        df = df[['groupID', 'system_prod', 'processor_prod', 'cpu_brand_prod', 'cpu_hz_prod', 'cpu_cores_prod', 'cpu_cores_total_prod', 'ram_total_prod', 'pricePerHour_prod', 'system_cons', 'processor_cons', 'cpu_brand_cons', 'cpu_hz_cons', 'cpu_cores_cons', 'cpu_cores_total_cons', 'ram_total_cons', 'pricePerHour_cons', 'machine_kafka', 'total', 'average', 'linger_ms', 'batch_size']]\n",
        "        \n",
        "\n",
        "        # Order the dataframe by groupID\n",
        "        df = df.sort_values(by=['groupID'])\n",
        "\n",
        "        # Clean the dataframe\n",
        "        # If average time is > 2 seconds, delete the next > 2 seconds\n",
        "\n",
        "        # Get the index of the rows to delete\n",
        "\n",
        "        # print the dataframe\n",
        "        # write the pandas dataframe to a csv file\n",
        "        df.to_csv('data/'+\"merge\"+file, index=False)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a csv file with all the scheduled events\n",
        "# use spark to read the csv file and create a dataframe\n",
        "from ast import literal_eval\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "# Get the list of files\n",
        "files = os.listdir('data')\n",
        "\n",
        "df_final = 0\n",
        "# Create a new file\n",
        "for file in files:\n",
        "\n",
        "    if file.startswith(\"merge\"):\n",
        "        df = spark.read.csv('data/'+file, header=True)\n",
        "\n",
        "        # count the number of rows\n",
        "        df = df.withColumn(\"count\", lit(1))\n",
        "        print(file+\" : \")\n",
        "        print((df.count()/1965)*100)\n",
        "        # add column percentage_success\n",
        "        if file.endswith(\"DS2_v2.csv\"):\n",
        "            df = df.withColumn(\"percentage_success\", lit((df.count()/1865)*100))\n",
        "        else:    \n",
        "            df = df.withColumn(\"percentage_success\", lit((df.count()/1965)*100))\n",
        "        # add the dataframe to the final dataframe\n",
        "        if df_final == 0:\n",
        "            df_final = df\n",
        "        else:\n",
        "            df_final = df_final.union(df)\n",
        "\n",
        "# remove columns groupID system_prod processor_prod cpu_brand_prod cpu_hz_prod cpu_cores_prod cpu_cores_total_prod ram_total_prod pricePerHour_prod system_cons processor_cons cpu_brand_cons cpu_hz_cons cpu_cores_cons cpu_cores_total_cons ram_total_cons pricePerHour_cons count\n",
        "df_final = df_final.drop('system_prod', 'processor_prod', 'cpu_brand_prod', 'cpu_hz_prod', 'cpu_cores_prod', 'cpu_cores_total_prod', 'ram_total_prod', 'pricePerHour_prod', 'system_cons', 'processor_cons', 'cpu_brand_cons', 'cpu_hz_cons', 'cpu_cores_cons', 'cpu_cores_total_cons', 'ram_total_cons', 'pricePerHour_cons', 'count')\n",
        "\n",
        "# add column producer and consumer with the values \"DS11_V2\"\n",
        "\n",
        "df_final = df_final.withColumn(\"producer\", lit('DS11_V2'))\n",
        "df_final = df_final.withColumn(\"consumer\", lit('DS11_V2'))\n",
        "\n",
        "# mixed the dataframe randomly\n",
        "df_final = df_final.toPandas().sample(frac=1)\n",
        "\n",
        "# write the dataframe to a csv file\n",
        "df_final.to_csv('data/'+\"merge_all.csv\", index=False)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "mergedataB2s.csv : \n85.85241730279898\nmergedataB4ms.csv : \n100.0\nmergedataD2as_v4.csv : \n92.264631043257\nmergedataD4s_v3.csv : \n100.0\nmergedataD8s_v3.csv : \n92.01017811704834\nmergedataDS2_v2.csv : \n99.59287531806615\n"
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a csv file with all the scheduled events\n",
        "# use spark to read the csv file and create a dataframe\n",
        "from ast import literal_eval\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "conf = SparkConf().setAppName(\"appName\").setMaster(\"local\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "spark = SparkSession(sc)\n",
        "\n",
        "\n",
        "# Get the list of files\n",
        "files = os.listdir('data')\n",
        "\n",
        "data = pd.read_csv('data/merge_all.csv')\n",
        "\n",
        "# Remove columns with label producer and consumer percentage_success\n",
        "data = data.drop(['producer', 'consumer', 'percentage_success'], axis=1)\n",
        "\n",
        "# Remove all the average time > 3\n",
        "\n",
        "\n",
        "\n",
        "machineKafkaTypeDict = {val:i for i, val in enumerate(data.machine_kafka.unique())}\n",
        "\n",
        "print(machineKafkaTypeDict)\n",
        "\n",
        "# map the machine_kafka to a number\n",
        "data['machine_kafka'] = data['machine_kafka'].map(machineKafkaTypeDict)\n",
        "\n",
        "data.head()\n",
        "\n",
        "# split the data into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.drop('average', axis=1), data['average'], test_size=0.20, random_state=10)\n",
        "\n",
        "regressor = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "regressor.fit(X_train, y_train)\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# calculate the mean absolute error\n",
        "print(mean_absolute_error(y_test, y_pred))\n",
        "\n",
        "# calculate the mean squared error\n",
        "print(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# calculate the accuracy\n",
        "print(regressor.score(X_test, y_test))\n",
        "\n",
        "# Show the feature importance\n",
        "print(regressor.feature_importances_)\n",
        "\n",
        "data.head()\n",
        "\n",
        "# add the column predicted to compare the results with the actual values\n",
        "data['predicted'] = regressor.predict(data.drop('average', axis=1))\n",
        "\n",
        "# show the 10 first rows\n",
        "data.head(10)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'D2as_v4': 0, 'B2s': 1, 'B4ms': 2, 'D8s_v3': 3, 'D4s_v3': 4, 'DS2_v2': 5}\n16.55423562701721\n1841.3467935236486\n0.6025500949993936\n[0.41698153 0.27929463 0.04769892 0.10372589 0.15229904]\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>groupID</th>\n      <th>machine_kafka</th>\n      <th>total</th>\n      <th>average</th>\n      <th>linger_ms</th>\n      <th>batch_size</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5549</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>42.108578</td>\n      <td>68</td>\n      <td>3317</td>\n      <td>29.835961</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8280</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.076891</td>\n      <td>922</td>\n      <td>2450</td>\n      <td>1.530219</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8058</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>83.866302</td>\n      <td>897</td>\n      <td>2279</td>\n      <td>60.512656</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>8784</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.117270</td>\n      <td>22178</td>\n      <td>1264</td>\n      <td>11.110191</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8077</td>\n      <td>0</td>\n      <td>52189.0</td>\n      <td>137.123093</td>\n      <td>897</td>\n      <td>2279</td>\n      <td>138.859381</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>9659</td>\n      <td>0</td>\n      <td>1756.0</td>\n      <td>58.520407</td>\n      <td>32693</td>\n      <td>4271</td>\n      <td>57.731211</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>10001</td>\n      <td>1</td>\n      <td>4757.0</td>\n      <td>171.160726</td>\n      <td>60040</td>\n      <td>2546</td>\n      <td>171.634143</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1973</td>\n      <td>1</td>\n      <td>6172.0</td>\n      <td>355.829865</td>\n      <td>33809</td>\n      <td>3357</td>\n      <td>335.754368</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9388</td>\n      <td>1</td>\n      <td>837.0</td>\n      <td>87.565107</td>\n      <td>4345</td>\n      <td>3308</td>\n      <td>15.977614</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1346</td>\n      <td>0</td>\n      <td>7949.0</td>\n      <td>35.713374</td>\n      <td>32</td>\n      <td>2906</td>\n      <td>35.196052</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "   groupID  machine_kafka    total     average  linger_ms  batch_size  \\\n0     5549              0      1.0   42.108578         68        3317   \n1     8280              0      1.0    0.076891        922        2450   \n2     8058              1      1.0   83.866302        897        2279   \n3     8784              1      1.0    0.117270      22178        1264   \n4     8077              0  52189.0  137.123093        897        2279   \n5     9659              0   1756.0   58.520407      32693        4271   \n6    10001              1   4757.0  171.160726      60040        2546   \n7     1973              1   6172.0  355.829865      33809        3357   \n8     9388              1    837.0   87.565107       4345        3308   \n9     1346              0   7949.0   35.713374         32        2906   \n\n    predicted  \n0   29.835961  \n1    1.530219  \n2   60.512656  \n3   11.110191  \n4  138.859381  \n5   57.731211  \n6  171.634143  \n7  335.754368  \n8   15.977614  \n9   35.196052  "
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}